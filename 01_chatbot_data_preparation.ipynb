{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 documents\n",
      "12 classes ['entertainment_movies', 'entertainment_music', 'flight_assistant', 'goodbye', 'greeting', 'options', 'order_dessert', 'order_drinks', 'order_food', 'tellmejoke', 'thanks', 'weather_search']\n",
      "119 unique lemmatized words [\"'s\", ',', 'a', 'again', 'album', 'all', 'am', 'amazing', 'anybody', 'anyone', 'are', 'assistant', 'at', 'available', 'beverage', 'bored', 'bot', 'boy', 'bro', 'bunch', 'bye', 'call', 'can', 'catch', 'cheer', 'cool', 'day', 'dessert', 'do', 'doubt', 'drink', 'eat', 'else', 'everybody', 'everything', 'farewell', 'flight', 'folk', 'food', 'for', 'give', 'good', 'goodbye', 'goodnight', 'great', 'have', 'hello', 'help', 'helpful', 'here', 'hey', 'hi', 'hola', 'how', 'howdy', 'i', 'in', 'is', 'it', 'item', 'joke', 'know', 'later', 'let', 'like', 'list', 'listen', 'long', 'me', 'menu', 'movie', 'much', 'music', \"n't\", 'navigate', 'need', 'nice', 'night', 'now', 'of', 'on', 'option', 'order', 'outside', 'play', 'please', 'possible', 'robot', 'see', 'show', 'so', 'some', 'something', 'song', 'super', 'sweet', 'talk', 'talking', 'tell', 'thank', 'thanks', 'that', 'the', 'then', 'there', 'this', 'to', 'today', 'wa', 'want', 'watch', 'weather', 'well', 'what', 'where', 'which', 'with', 'ya', 'you']\n"
     ]
    }
   ],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# lemmaztize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # create our training data\n",
    "# training = []\n",
    "# # create an empty array for our output\n",
    "# output_empty = [0] * len(classes)\n",
    "# # training set, bag of words for each sentence\n",
    "# for doc in documents:\n",
    "#     # initialize our bag of words\n",
    "#     bag = []\n",
    "#     # list of tokenized words for the pattern\n",
    "#     print(doc[0])\n",
    "#     pattern_words = doc[0]\n",
    "#     # lemmatize each word - create base word, in attempt to represent related words\n",
    "#     pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "#     # create our bag of words array with 1, if word match found in current pattern\n",
    "#     for w in words:\n",
    "#         bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "#     # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "#     output_row = list(output_empty)\n",
    "# #     print(output_row)\n",
    "#     output_row[classes.index(doc[1])] = 1\n",
    "# #     print(classes.index(doc[1]))\n",
    "# #     print(output_row)\n",
    "#     training.append([bag, output_row])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Data preprocessing steps\n",
    "    \"\"\"\n",
    "    def __init__(self, words , classes):\n",
    "        self.words = words\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.classes = classes\n",
    "        \n",
    "    def transform(self, docs, *_):\n",
    "        transformed_data = []\n",
    "        # create an empty array for our output\n",
    "        output_empty = [0] * len(self.classes)\n",
    "        for doc in docs:\n",
    "            # initialize our bag of words\n",
    "            bag = []\n",
    "            # list of tokenized words for the pattern\n",
    "            #    print(doc[0])\n",
    "            pattern_words = doc[0]\n",
    "            # lemmatize each word - create base word, in attempt to represent related words\n",
    "            pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "            # create our bag of words array with 1, if word match found in current pattern\n",
    "            for w in self.words:\n",
    "                bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "            # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "            output_row = list(output_empty)\n",
    "            output_row[self.classes.index(doc[1])] = 1\n",
    "    \n",
    "            transformed_data.append([bag, output_row])\n",
    "        \n",
    "        random.shuffle(transformed_data)\n",
    "        return np.array(transformed_data, dtype=object)\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0])\n",
      " list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "datapreparer = DataTransformer(words = words, classes = classes)\n",
    "training = datapreparer.transform(documents)\n",
    "print(training[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # save all of our data structures\n",
    "# print(\"Saving training data\")\n",
    "# pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data.parquet\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dataprep',\n",
       "                 DataTransformer(classes=['entertainment_movies',\n",
       "                                          'entertainment_music',\n",
       "                                          'flight_assistant', 'goodbye',\n",
       "                                          'greeting', 'options',\n",
       "                                          'order_dessert', 'order_drinks',\n",
       "                                          'order_food', 'tellmejoke', 'thanks',\n",
       "                                          'weather_search'],\n",
       "                                 words=[\"'s\", ',', 'a', 'again', 'album', 'all',\n",
       "                                        'am', 'amazing', 'anybody', 'anyone',\n",
       "                                        'are', 'assistant', 'at', 'available',\n",
       "                                        'beverage', 'bored', 'bot', 'boy',\n",
       "                                        'bro', 'bunch', 'bye', 'call', 'can',\n",
       "                                        'catch', 'cheer', 'cool', 'day',\n",
       "                                        'dessert', 'do', 'doubt', ...]))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataprep_pipeline = Pipeline([\n",
    "    ('dataprep', datapreparer)\n",
    "])\n",
    "\n",
    "dataprep_pipeline.fit(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import util\n",
    "util.serialize_to(dataprep_pipeline, \"chatbot_data_preparation.sav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
